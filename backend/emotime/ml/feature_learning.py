"""
üß†üî¨ Deep Feature Learning ‚Äî MIT-style advanced feature extraction

Advanced multi-modal feature learning:
- Autoencoder-based feature extraction  
- Attention mechanisms for temporal patterns
- Cross-modal feature fusion
- Representation learning with self-supervision

Based on MIT CSAIL research in representation learning.
"""

import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
from collections import deque
import json

try:
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.cluster import KMeans
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False

from ..sensors import SensorData, SensorType, TextData, TouchData, AudioData
from ..fusion import EmotionalFeatures
from ..utils import safe_logger


@dataclass
class LearnedFeature:
    """–ò–∑—É—á–µ–Ω–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏."""
    name: str
    values: np.ndarray
    importance: float
    temporal_pattern: Dict[str, float]
    modality_weights: Dict[str, float]
    created_at: datetime


class AttentionMechanism:
    """
    –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π.
    
    –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è self-attention –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è
    –≤–∞–∂–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ –≤ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö.
    """
    
    def __init__(self, feature_dim: int):
        self.feature_dim = feature_dim
        self.attention_weights = None
        
    def compute_attention(self, sequence: np.ndarray) -> np.ndarray:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
        
        Args:
            sequence: (seq_len, feature_dim) –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            
        Returns:
            attention_weights: (seq_len,) –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è
        """
        if len(sequence) == 0:
            return np.array([])
            
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è attention
        # Q, K, V = sequence –¥–ª—è self-attention
        seq_len = len(sequence)
        
        # –í—ã—á–∏—Å–ª—è–µ–º similarity matrix
        similarity = np.dot(sequence, sequence.T)  # (seq_len, seq_len)
        
        # Softmax –ø–æ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ
        attention_scores = np.exp(similarity) / np.sum(np.exp(similarity), axis=1, keepdims=True)
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è
        attention_weights = np.mean(attention_scores, axis=1)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        attention_weights = attention_weights / np.sum(attention_weights)
        
        self.attention_weights = attention_weights
        return attention_weights
    
    def apply_attention(self, sequence: np.ndarray, weights: np.ndarray = None) -> np.ndarray:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        if weights is None:
            weights = self.compute_attention(sequence)
            
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
        attended_features = np.average(sequence, axis=0, weights=weights)
        return attended_features


class CrossModalFusion:
    """
    –ö—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π (text, audio, touch)
    —Å —É—á–µ—Ç–æ–º –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π.
    """
    
    def __init__(self):
        self.modality_weights = {
            SensorType.TEXT: 0.4,
            SensorType.AUDIO: 0.35, 
            SensorType.TOUCH: 0.25
        }
        self.interaction_matrix = None
        
    def learn_interactions(self, multi_modal_data: List[Dict[str, np.ndarray]]):
        """–ò–∑—É—á–∞–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏."""
        if not multi_modal_data:
            return
            
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º
        modality_features = {modality: [] for modality in SensorType}
        
        for data_point in multi_modal_data:
            for modality, features in data_point.items():
                if isinstance(modality, str):
                    modality = SensorType(modality)
                modality_features[modality].append(features)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏
        interactions = {}
        
        for mod1 in SensorType:
            if not modality_features[mod1]:
                continue
                
            for mod2 in SensorType:
                if not modality_features[mod2] or mod1 == mod2:
                    continue
                    
                try:
                    # –ü—Ä–æ—Å—Ç–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É —Å—Ä–µ–¥–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                    feat1 = np.array(modality_features[mod1])
                    feat2 = np.array(modality_features[mod2])
                    
                    if feat1.ndim > 1:
                        feat1 = np.mean(feat1, axis=1)
                    if feat2.ndim > 1:
                        feat2 = np.mean(feat2, axis=1)
                        
                    min_len = min(len(feat1), len(feat2))
                    if min_len > 1:
                        correlation = np.corrcoef(feat1[:min_len], feat2[:min_len])[0, 1]
                        if not np.isnan(correlation):
                            interactions[(mod1.value, mod2.value)] = abs(correlation)
                            
                except Exception as e:
                    safe_logger.warning(f"Interaction calculation failed: {e}")
        
        self.interaction_matrix = interactions
        safe_logger.info(f"Learned {len(interactions)} modality interactions")
    
    def fuse_features(self, modal_features: Dict[SensorType, np.ndarray]) -> np.ndarray:
        """–°–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å —É—á–µ—Ç–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π."""
        if not modal_features:
            return np.array([])
        
        # –ë–∞–∑–æ–≤–æ–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ
        fused = []
        total_weight = 0
        
        for modality, features in modal_features.items():
            weight = self.modality_weights.get(modality, 0.1)
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å –µ—Å–ª–∏ –µ—Å—Ç—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏
            if self.interaction_matrix:
                interaction_boost = 0
                for (mod1, mod2), strength in self.interaction_matrix.items():
                    if mod1 == modality.value:
                        if SensorType(mod2) in modal_features:
                            interaction_boost += strength * 0.1
                
                weight += interaction_boost
            
            if len(features.shape) == 1:
                fused.append(features * weight)
            else:
                fused.append(np.mean(features, axis=0) * weight)
                
            total_weight += weight
        
        if not fused:
            return np.array([])
            
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ –æ–±—â–µ–º—É –≤–µ—Å—É
        result = np.sum(fused, axis=0) / max(total_weight, 1e-6)
        return result


class DeepFeatureLearner:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∏–∑—É—á–∞—Ç–µ–ª—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    
    MIT-level features:
    - Multi-modal representation learning
    - Temporal attention mechanisms
    - Cross-modal feature fusion
    - Unsupervised feature discovery
    """
    
    def __init__(
        self,
        user_id: str,
        feature_dim: int = 64,
        temporal_window: int = 50,
        learning_rate: float = 0.001
    ):
        self.user_id = user_id
        self.feature_dim = feature_dim
        self.temporal_window = temporal_window
        self.learning_rate = learning_rate
        
        # Components
        self.attention = AttentionMechanism(feature_dim)
        self.fusion = CrossModalFusion()
        
        # Feature storage
        self.temporal_buffer = deque(maxlen=temporal_window)
        self.learned_features = {}
        
        # Dimensionality reduction
        self.pca = PCA(n_components=min(feature_dim, 10)) if ML_AVAILABLE else None
        self.scaler = MinMaxScaler() if ML_AVAILABLE else None
        
        # Pattern discovery
        self.pattern_clusters = None
        self.cluster_model = KMeans(n_clusters=6, random_state=42) if ML_AVAILABLE else None
        
        safe_logger.info(f"Deep feature learner initialized for user {user_id}")
    
    def extract_text_features(self, text_data: TextData) -> np.ndarray:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–ª—É–±–æ–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        features = []
        
        text = text_data.text.lower()
        
        # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features.extend([
            len(text),  # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
            text_data.word_count,
            text_data.char_count,
            text_data.word_count / max(text_data.char_count, 1),  # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≤
        ])
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
        positive_words = sum(1 for word in ["happy", "joy", "love", "great", "amazing", "wonderful", "—Ä–∞–¥–æ—Å—Ç—å", "—Å—á–∞—Å—Ç—å–µ", "—Å—É–ø–µ—Ä", "–æ—Ç–ª–∏—á–Ω–æ"] if word in text)
        negative_words = sum(1 for word in ["sad", "angry", "hate", "terrible", "awful", "–≥—Ä—É—Å—Ç—å", "–∑–ª–æ—Å—Ç—å", "—É–∂–∞—Å–Ω–æ", "–ø–ª–æ—Ö–æ"] if word in text)
        stress_words = sum(1 for word in ["urgent", "pressure", "deadline", "stress", "—Å—Ä–æ—á–Ω–æ", "–¥–∞–≤–ª–µ–Ω–∏–µ", "—Å—Ç—Ä–µ—Å—Å"] if word in text)
        calm_words = sum(1 for word in ["calm", "peace", "relax", "quiet", "—Å–ø–æ–∫–æ–π–Ω–æ", "—Ç–∏—Ö–æ", "—Ä–∞—Å—Å–ª–∞–±–ª–µ–Ω–Ω–æ"] if word in text)
        
        features.extend([positive_words, negative_words, stress_words, calm_words])
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        exclamation_count = text.count('!')
        question_count = text.count('?')
        caps_ratio = sum(1 for c in text if c.isupper()) / max(len(text), 1)
        
        features.extend([exclamation_count, question_count, caps_ratio])
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if text_data.typing_speed:
            features.append(text_data.typing_speed)
        else:
            features.append(0.5)  # Default
            
        if text_data.pause_duration:
            features.append(min(text_data.pause_duration, 10) / 10)  # Normalized
        else:
            features.append(0.5)
        
        return np.array(features, dtype=np.float32)
    
    def extract_audio_features(self, audio_data: AudioData) -> np.ndarray:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö."""
        features = [
            audio_data.pitch_mean,
            audio_data.pitch_variance,
            audio_data.speech_rate / 200.0,  # Normalize to ~[0,1]
            audio_data.volume_level,
            audio_data.pause_ratio,
            len(audio_data.emotion_markers),
        ]
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã –∫–∞–∫ one-hot
        emotion_categories = ["calm", "joy", "stress", "anger", "sadness", "focus"]
        for category in emotion_categories:
            features.append(1.0 if category in audio_data.emotion_markers else 0.0)
        
        return np.array(features, dtype=np.float32)
    
    def extract_touch_features(self, touch_data: TouchData) -> np.ndarray:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Ç–∞–∫—Ç–∏–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        features = [
            touch_data.pressure,
            touch_data.duration,
            min(touch_data.frequency / 10.0, 1.0),  # Normalize frequency
        ]
        
        # Pattern encoding
        pattern_encoding = {
            "tap": [1, 0, 0, 0],
            "swipe": [0, 1, 0, 0], 
            "hold": [0, 0, 1, 0],
            "gesture": [0, 0, 0, 1],
            "rapid_taps": [1, 1, 0, 0],
            "gentle": [0, 0, 1, 1]
        }
        
        pattern_features = pattern_encoding.get(touch_data.pattern, [0, 0, 0, 0])
        features.extend(pattern_features)
        
        # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        if touch_data.coordinates:
            features.extend([touch_data.coordinates[0], touch_data.coordinates[1]])
        else:
            features.extend([0.5, 0.5])  # Default center
        
        return np.array(features, dtype=np.float32)
    
    async def process_sensor_batch(
        self, 
        sensor_data_list: List[SensorData]
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á —Å–µ–Ω—Å–æ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –≥–ª—É–±–æ–∫–∏–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
        
        modal_features = {}
        feature_metadata = {
            "timestamp": datetime.now(),
            "modalities_present": [],
            "attention_weights": {},
            "cross_modal_interactions": {}
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º
        for sensor_data in sensor_data_list:
            modality = sensor_data.sensor_type
            raw_data = sensor_data.raw_data
            
            try:
                if modality == SensorType.TEXT and isinstance(raw_data, TextData):
                    features = self.extract_text_features(raw_data)
                elif modality == SensorType.AUDIO and isinstance(raw_data, AudioData):
                    features = self.extract_audio_features(raw_data)
                elif modality == SensorType.TOUCH and isinstance(raw_data, TouchData):
                    features = self.extract_touch_features(raw_data)
                else:
                    continue
                
                modal_features[modality] = features
                feature_metadata["modalities_present"].append(modality.value)
                
            except Exception as e:
                safe_logger.warning(f"Feature extraction failed for {modality}: {e}")
        
        if not modal_features:
            return np.array([]), feature_metadata
        
        # Cross-modal fusion
        fused_features = self.fusion.fuse_features(modal_features)
        
        if len(fused_features) == 0:
            return np.array([]), feature_metadata
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –±—É—Ñ–µ—Ä—É
        self.temporal_buffer.append({
            "features": fused_features,
            "timestamp": datetime.now(),
            "modalities": list(modal_features.keys())
        })
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º temporal attention –µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
        if len(self.temporal_buffer) > 5:
            # –°–æ–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            sequence = np.array([item["features"] for item in list(self.temporal_buffer)[-10:]])
            
            # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            min_dim = min(len(feat) for feat in sequence)
            if min_dim > 0:
                sequence = np.array([feat[:min_dim] for feat in sequence])
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º attention
                attention_weights = self.attention.compute_attention(sequence)
                attended_features = self.attention.apply_attention(sequence, attention_weights)
                
                feature_metadata["attention_weights"] = attention_weights.tolist()
                fused_features = attended_features
        
        # Dimensionality reduction –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if ML_AVAILABLE and self.pca and len(fused_features) > self.feature_dim:
            if not hasattr(self.pca, 'components_'):
                # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è PCA
                if len(self.temporal_buffer) >= 10:
                    training_data = np.array([item["features"][:len(fused_features)] for item in self.temporal_buffer])
                    try:
                        self.pca.fit(training_data)
                        self.scaler.fit(training_data)
                    except Exception as e:
                        safe_logger.warning(f"PCA training failed: {e}")
            
            if hasattr(self.pca, 'components_'):
                try:
                    fused_features = fused_features.reshape(1, -1)
                    fused_features = self.scaler.transform(fused_features)
                    fused_features = self.pca.transform(fused_features)[0]
                except Exception as e:
                    safe_logger.warning(f"PCA transform failed: {e}")
        
        # Pattern discovery
        await self._discover_patterns(fused_features, feature_metadata)
        
        return fused_features, feature_metadata
    
    async def _discover_patterns(self, features: np.ndarray, metadata: Dict):
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö."""
        if not ML_AVAILABLE or len(self.temporal_buffer) < 20:
            return
        
        try:
            # –°–æ–±–∏—Ä–∞–µ–º –Ω–µ–¥–∞–≤–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            recent_features = []
            for item in list(self.temporal_buffer)[-20:]:
                feat = item["features"]
                if len(feat) == len(features):  # –û–¥–∏–Ω–∞–∫–æ–≤–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
                    recent_features.append(feat)
            
            if len(recent_features) < 10:
                return
            
            # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            feature_matrix = np.array(recent_features)
            
            if not hasattr(self.cluster_model, 'cluster_centers_'):
                self.cluster_model.fit(feature_matrix)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä –¥–ª—è —Ç–µ–∫—É—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            cluster_id = self.cluster_model.predict([features])[0]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞—Ç—Ç–µ—Ä–Ω–µ
            metadata["discovered_pattern"] = {
                "cluster_id": int(cluster_id),
                "cluster_centers_count": len(self.cluster_model.cluster_centers_)
            }
            
        except Exception as e:
            safe_logger.warning(f"Pattern discovery failed: {e}")
    
    def get_learned_representations(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–∑—É—á–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è."""
        return {
            "user_id": self.user_id,
            "temporal_buffer_size": len(self.temporal_buffer),
            "learned_features_count": len(self.learned_features),
            "pca_explained_variance": self.pca.explained_variance_ratio_.tolist() if (ML_AVAILABLE and self.pca and hasattr(self.pca, 'explained_variance_ratio_')) else [],
            "attention_mechanisms": {
                "last_weights": self.attention.attention_weights.tolist() if self.attention.attention_weights is not None else []
            },
            "cross_modal_fusion": {
                "modality_weights": {k.value: v for k, v in self.fusion.modality_weights.items()},
                "interaction_matrix": self.fusion.interaction_matrix or {}
            },
            "pattern_clusters": {
                "n_clusters": self.cluster_model.n_clusters if (ML_AVAILABLE and self.cluster_model) else 0,
                "cluster_centers_available": hasattr(self.cluster_model, 'cluster_centers_') if (ML_AVAILABLE and self.cluster_model) else False
            }
        }
    
    async def update_cross_modal_learning(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ."""
        if len(self.temporal_buffer) < 10:
            return
        
        # –°–æ–±–∏—Ä–∞–µ–º –º—É–ª—å—Ç–∏-–º–æ–¥–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        multi_modal_data = []
        
        for item in self.temporal_buffer:
            if "modalities" in item:
                data_point = {}
                # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º
                # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
                data_point[item["modalities"][0]] = item["features"]
                multi_modal_data.append(data_point)
        
        # –û–±—É—á–∞–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        self.fusion.learn_interactions(multi_modal_data)
        safe_logger.info("Cross-modal learning updated")


# Global instances
_feature_learners = {}

def get_feature_learner(user_id: str) -> DeepFeatureLearner:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç feature learner –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    if user_id not in _feature_learners:
        _feature_learners[user_id] = DeepFeatureLearner(user_id)
    return _feature_learners[user_id]