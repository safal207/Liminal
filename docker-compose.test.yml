version: '3.8'

services:
  # Redis для кэширования
  redis:
    image: redis:7-alpine
    container_name: resonance-redis-test
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Основной backend с ML
  backend:
    build: 
      context: ./backend
      dockerfile: Dockerfile
    container_name: resonance-backend-test
    ports:
      - "8000:8000"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - OPENAI_API_KEY=${OPENAI_API_KEY:-sk-test-key}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-sk-ant-test-key}
      - XAI_CACHE_SIZE=1000
      - XAI_ENABLE_SHAP=true
      - XAI_ENABLE_LIME=true
      - OPENAI_MODEL=gpt-4-turbo-preview
      - OPENAI_MAX_TOKENS=2000
      - OPENAI_TEMPERATURE=0.3
      - CLAUDE_MODEL=claude-3-sonnet-20240229
      - CLAUDE_MAX_TOKENS=4000
      - CLAUDE_TEMPERATURE=0.1
      - MULTI_LLM_FALLBACK_ENABLED=true
      - MULTI_LLM_CONSENSUS_THRESHOLD=0.7
      - MULTI_LLM_COST_OPTIMIZATION=true
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - ./backend:/app
      - ml_models:/app/ml/models

volumes:
  redis_data:
  ml_models:

networks:
  default:
    name: resonance-test-network
