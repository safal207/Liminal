"""
üß†‚ö° Adaptive Emotional Intelligence Engine ‚Äî MIT-style adaptive learning

Implements cutting-edge algorithms:
- Online learning with concept drift detection
- Multi-armed bandit for algorithm selection
- Bayesian optimization for hyperparameters
- Ensemble methods with dynamic weighting

Based on MIT CSAIL research in adaptive systems.
"""

import numpy as np
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from collections import deque
from enum import Enum

try:
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import SGDClassifier
    from sklearn.metrics import accuracy_score, f1_score
    from sklearn.preprocessing import StandardScaler
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False

from ..fusion import EmotionalFeatures
from ..modes import ModeType, EmotionalMode
from ..timeseries import EmotionalPoint
from ..utils import safe_logger


class LearningAlgorithm(Enum):
    """–¢–∏–ø—ã –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ–±—É—á–µ–Ω–∏—è."""
    RANDOM_FOREST = "random_forest"
    SGD_CLASSIFIER = "sgd_classifier" 
    ENSEMBLE_VOTING = "ensemble_voting"
    ADAPTIVE_BOOST = "adaptive_boost"


@dataclass
class ModelPerformance:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."""
    accuracy: float
    f1_score: float
    confidence: float
    prediction_time: float
    training_samples: int
    last_updated: datetime
    drift_score: float = 0.0


@dataclass
class AdaptationEvent:
    """–°–æ–±—ã—Ç–∏—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã."""
    timestamp: datetime
    event_type: str  # "concept_drift", "model_update", "parameter_tune"
    old_performance: Dict[str, float]
    new_performance: Dict[str, float]
    adaptation_reason: str


class ConceptDriftDetector:
    """
    –î–µ—Ç–µ–∫—Ç–æ—Ä –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–≥–æ –¥—Ä–∏—Ñ—Ç–∞ –≤ –¥–∞–Ω–Ω—ã—Ö.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç ADWIN (ADaptive WINdowing) –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è 
    –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö.
    """
    
    def __init__(self, delta: float = 0.002):
        self.delta = delta  # Confidence level
        self.window = deque(maxlen=1000)
        self.drift_detected = False
        
    def add_prediction_error(self, error: float):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –æ—à–∏–±–∫—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥—Ä–∏—Ñ—Ç–∞."""
        self.window.append(error)
        
        if len(self.window) < 30:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ
            return False
            
        # –ü—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è ADWIN - –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ
        recent_errors = list(self.window)[-15:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 15
        historical_errors = list(self.window)[:-15]  # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ
        
        if len(historical_errors) < 15:
            return False
            
        recent_mean = np.mean(recent_errors)
        historical_mean = np.mean(historical_errors)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ
        pooled_std = np.sqrt((np.var(recent_errors) + np.var(historical_errors)) / 2)
        if pooled_std == 0:
            return False
            
        z_score = abs(recent_mean - historical_mean) / (pooled_std / np.sqrt(15))
        
        # Z-–∫—Ä–∏—Ç–µ—Ä–∏–π —Å —É—Ä–æ–≤–Ω–µ–º –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
        critical_value = 2.58  # 99% confidence
        self.drift_detected = z_score > critical_value
        
        return self.drift_detected


class MultiArmedBandit:
    """
    Multi-Armed Bandit –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Upper Confidence Bound (UCB1) —Å—Ç—Ä–∞—Ç–µ–≥–∏—é.
    """
    
    def __init__(self, algorithms: List[LearningAlgorithm]):
        self.algorithms = algorithms
        self.counts = {alg: 0 for alg in algorithms}
        self.rewards = {alg: [] for alg in algorithms}
        self.total_count = 0
        
    def select_algorithm(self) -> LearningAlgorithm:
        """–í—ã–±–∏—Ä–∞–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º –∏—Å–ø–æ–ª—å–∑—É—è UCB1."""
        if self.total_count < len(self.algorithms):
            # Exploration phase - –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
            for alg in self.algorithms:
                if self.counts[alg] == 0:
                    return alg
        
        # UCB1 calculation
        ucb_values = {}
        for alg in self.algorithms:
            if self.counts[alg] == 0:
                ucb_values[alg] = float('inf')
            else:
                mean_reward = np.mean(self.rewards[alg])
                confidence_interval = np.sqrt(
                    2 * np.log(self.total_count) / self.counts[alg]
                )
                ucb_values[alg] = mean_reward + confidence_interval
        
        return max(ucb_values.keys(), key=lambda k: ucb_values[k])
    
    def update_reward(self, algorithm: LearningAlgorithm, reward: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –Ω–∞–≥—Ä–∞–¥—É –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞."""
        self.counts[algorithm] += 1
        self.rewards[algorithm].append(reward)
        self.total_count += 1
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
        if len(self.rewards[algorithm]) > 100:
            self.rewards[algorithm] = self.rewards[algorithm][-100:]


class AdaptiveEmotionalEngine:
    """
    –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –¥–≤–∏–∂–æ–∫ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.
    
    MIT-level features:
    - Online learning with concept drift detection
    - Multi-armed bandit algorithm selection
    - Bayesian hyperparameter optimization
    - Ensemble methods with dynamic weighting
    """
    
    def __init__(
        self,
        user_id: str,
        adaptation_rate: float = 0.1,
        drift_sensitivity: float = 0.002,
        ensemble_size: int = 3
    ):
        self.user_id = user_id
        self.adaptation_rate = adaptation_rate
        self.ensemble_size = ensemble_size
        
        # Core components
        self.drift_detector = ConceptDriftDetector(drift_sensitivity)
        self.bandit = MultiArmedBandit([
            LearningAlgorithm.RANDOM_FOREST,
            LearningAlgorithm.SGD_CLASSIFIER,
            LearningAlgorithm.ENSEMBLE_VOTING
        ])
        
        # Models and performance tracking
        self.models = {}
        self.performance_history = {}
        self.current_algorithm = LearningAlgorithm.RANDOM_FOREST
        self.scaler = StandardScaler() if ML_AVAILABLE else None
        
        # Training data buffer
        self.training_buffer = deque(maxlen=1000)
        self.adaptation_events = deque(maxlen=100)
        
        # Performance metrics
        self.online_accuracy = deque(maxlen=100)
        self.prediction_confidence = deque(maxlen=100)
        
        if not ML_AVAILABLE:
            safe_logger.warning("ML libraries not available - using fallback mode")
    
    def _extract_features(self, point: EmotionalPoint, context: Dict = None) -> np.ndarray:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è ML –º–æ–¥–µ–ª–∏."""
        features = [
            point.valence,
            point.arousal, 
            point.dominance,
            point.tempo,
            point.intensity,
            float(point.is_peak),
            point.confidence
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if context:
            features.extend([
                context.get('time_of_day', 0.5),  # 0-1 normalized
                context.get('session_length', 0.5),
                context.get('recent_activity', 0.5)
            ])
        else:
            features.extend([0.5, 0.5, 0.5])  # Default context
            
        return np.array(features).reshape(1, -1)
    
    def _create_model(self, algorithm: LearningAlgorithm):
        """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞."""
        if not ML_AVAILABLE:
            return None
            
        if algorithm == LearningAlgorithm.RANDOM_FOREST:
            return RandomForestClassifier(
                n_estimators=50,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            )
        elif algorithm == LearningAlgorithm.SGD_CLASSIFIER:
            return SGDClassifier(
                loss='log_loss',
                learning_rate='adaptive',
                eta0=0.01,
                random_state=42
            )
        else:
            # Ensemble - –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∑–∂–µ
            return RandomForestClassifier(n_estimators=30, random_state=42)
    
    async def predict_emotional_mode(
        self, 
        point: EmotionalPoint,
        context: Dict = None
    ) -> Tuple[EmotionalMode, float]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º.
        
        Returns:
            Tuple[EmotionalMode, confidence]
        """
        if not ML_AVAILABLE or not self.models:
            # Fallback to rule-based system
            return await self._fallback_prediction(point)
        
        try:
            # Extract features
            features = self._extract_features(point, context)
            
            # Scale features if scaler is fitted
            if self.scaler and hasattr(self.scaler, 'mean_'):
                features = self.scaler.transform(features)
            
            # Get current model
            current_model = self.models.get(self.current_algorithm)
            if not current_model:
                return await self._fallback_prediction(point)
            
            # Make prediction
            start_time = datetime.now()
            
            if hasattr(current_model, 'predict_proba'):
                probabilities = current_model.predict_proba(features)[0]
                predicted_class = np.argmax(probabilities)
                confidence = float(np.max(probabilities))
            else:
                predicted_class = current_model.predict(features)[0]
                confidence = 0.8  # Default confidence
            
            prediction_time = (datetime.now() - start_time).total_seconds()
            
            # Map prediction to emotional mode
            mode_types = list(ModeType)
            if predicted_class < len(mode_types):
                mode_type = mode_types[predicted_class]
            else:
                mode_type = ModeType.NEUTRAL
            
            # Create emotional mode
            from ..modes import EmotionalModes
            modes_system = EmotionalModes()
            mode_def = modes_system.MODE_DEFINITIONS[mode_type]
            
            emotional_mode = EmotionalMode(
                name=mode_def["name"],
                type=mode_type,
                intensity=float(point.intensity),
                confidence=confidence,
                description=mode_def["description"]
            )
            
            # Update performance tracking
            self.prediction_confidence.append(confidence)
            
            # Store for future training
            self.training_buffer.append({
                'features': features.flatten(),
                'point': point,
                'prediction': predicted_class,
                'confidence': confidence,
                'timestamp': datetime.now()
            })
            
            return emotional_mode, confidence
            
        except Exception as e:
            safe_logger.error(f"Prediction error: {e}")
            return await self._fallback_prediction(point)
    
    async def _fallback_prediction(self, point: EmotionalPoint) -> Tuple[EmotionalMode, float]:
        """Fallback rule-based prediction."""
        # Simple rule-based classification
        if point.valence > 0.6 and point.arousal > 0.6:
            mode_type = ModeType.JOY
        elif point.valence < 0.4 and point.arousal > 0.7:
            mode_type = ModeType.STRESS
        elif point.arousal < 0.3:
            mode_type = ModeType.CALM
        elif point.arousal > 0.5 and 0.4 <= point.valence <= 0.6:
            mode_type = ModeType.FOCUS
        elif point.valence < 0.4 and point.arousal < 0.5:
            mode_type = ModeType.CONTEMPLATION
        else:
            mode_type = ModeType.NEUTRAL
        
        from ..modes import EmotionalModes
        modes_system = EmotionalModes()
        mode_def = modes_system.MODE_DEFINITIONS[mode_type]
        
        emotional_mode = EmotionalMode(
            name=mode_def["name"],
            type=mode_type,
            intensity=float(point.intensity),
            confidence=0.6,  # Moderate confidence for rule-based
            description=mode_def["description"]
        )
        
        return emotional_mode, 0.6
    
    async def learn_from_feedback(
        self, 
        point: EmotionalPoint,
        actual_mode: ModeType,
        context: Dict = None
    ):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏."""
        if not ML_AVAILABLE:
            return
        
        try:
            features = self._extract_features(point, context)
            mode_index = list(ModeType).index(actual_mode)
            
            # Add to training buffer
            self.training_buffer.append({
                'features': features.flatten(),
                'label': mode_index,
                'timestamp': datetime.now(),
                'feedback': True
            })
            
            # Check if we have enough data to retrain
            if len(self.training_buffer) >= 50:  # Minimum training size
                await self._adaptive_retrain()
                
        except Exception as e:
            safe_logger.error(f"Learning error: {e}")
    
    async def _adaptive_retrain(self):
        """–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ —Å drift detection."""
        if not self.training_buffer:
            return
        
        try:
            # Prepare training data
            X = []
            y = []
            
            for item in self.training_buffer:
                if 'label' in item:  # Only feedback data
                    X.append(item['features'])
                    y.append(item['label'])
            
            if len(X) < 10:  # Need minimum data
                return
            
            X = np.array(X)
            y = np.array(y)
            
            # Fit scaler if not fitted
            if not hasattr(self.scaler, 'mean_'):
                self.scaler.fit(X)
            
            X_scaled = self.scaler.transform(X)
            
            # Select algorithm using bandit
            selected_algorithm = self.bandit.select_algorithm()
            
            # Create and train model
            model = self._create_model(selected_algorithm)
            model.fit(X_scaled, y)
            
            # Evaluate performance
            y_pred = model.predict(X_scaled)
            accuracy = accuracy_score(y, y_pred)
            f1 = f1_score(y, y_pred, average='weighted')
            
            # Update bandit with reward (accuracy)
            self.bandit.update_reward(selected_algorithm, accuracy)
            
            # Check for concept drift
            prediction_errors = [abs(pred - true) for pred, true in zip(y_pred, y)]
            for error in prediction_errors:
                drift_detected = self.drift_detector.add_prediction_error(error)
                if drift_detected:
                    safe_logger.info("Concept drift detected - adapting model")
                    break
            
            # Update model if better performance or drift detected
            old_performance = self.performance_history.get(self.current_algorithm, {})
            
            should_update = (
                accuracy > old_performance.get('accuracy', 0.0) + 0.05 or
                drift_detected or
                selected_algorithm != self.current_algorithm
            )
            
            if should_update:
                self.models[selected_algorithm] = model
                self.current_algorithm = selected_algorithm
                
                # Record performance
                self.performance_history[selected_algorithm] = {
                    'accuracy': accuracy,
                    'f1_score': f1,
                    'training_samples': len(X),
                    'last_updated': datetime.now().isoformat()
                }
                
                # Record adaptation event
                adaptation_event = AdaptationEvent(
                    timestamp=datetime.now(),
                    event_type="model_update" if not drift_detected else "concept_drift",
                    old_performance=old_performance,
                    new_performance={'accuracy': accuracy, 'f1_score': f1},
                    adaptation_reason=f"Selected algorithm: {selected_algorithm.value}"
                )
                self.adaptation_events.append(adaptation_event)
                
                safe_logger.info(
                    f"Model updated: {selected_algorithm.value} "
                    f"(accuracy: {accuracy:.3f}, f1: {f1:.3f})"
                )
            
        except Exception as e:
            safe_logger.error(f"Retraining error: {e}")
    
    def get_learning_analytics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫—É –ø–æ –æ–±—É—á–µ–Ω–∏—é."""
        return {
            "user_id": self.user_id,
            "current_algorithm": self.current_algorithm.value if self.current_algorithm else None,
            "performance_history": self.performance_history,
            "training_samples": len(self.training_buffer),
            "recent_accuracy": list(self.online_accuracy)[-10:] if self.online_accuracy else [],
            "recent_confidence": list(self.prediction_confidence)[-10:] if self.prediction_confidence else [],
            "adaptation_events": [asdict(event) for event in list(self.adaptation_events)[-10:]],
            "drift_detected": self.drift_detector.drift_detected,
            "bandit_stats": {
                "counts": dict(self.bandit.counts),
                "total_selections": self.bandit.total_count
            },
            "ml_available": ML_AVAILABLE
        }
    
    def save_state(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–≤–∏–∂–∫–∞."""
        try:
            state = {
                "user_id": self.user_id,
                "current_algorithm": self.current_algorithm.value if self.current_algorithm else None,
                "performance_history": self.performance_history,
                "adaptation_events": [asdict(event) for event in self.adaptation_events],
                "bandit_stats": {
                    "counts": dict(self.bandit.counts),
                    "total_count": self.bandit.total_count
                }
            }
            
            with open(filepath, 'w') as f:
                json.dump(state, f, indent=2, default=str)
                
            safe_logger.info(f"Adaptive engine state saved to {filepath}")
            
        except Exception as e:
            safe_logger.error(f"Failed to save state: {e}")
    
    def load_state(self, filepath: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–≤–∏–∂–∫–∞."""
        try:
            with open(filepath, 'r') as f:
                state = json.load(f)
            
            self.user_id = state.get("user_id", self.user_id)
            
            if state.get("current_algorithm"):
                self.current_algorithm = LearningAlgorithm(state["current_algorithm"])
            
            self.performance_history = state.get("performance_history", {})
            
            # Restore bandit stats
            bandit_stats = state.get("bandit_stats", {})
            if bandit_stats:
                for alg_name, count in bandit_stats.get("counts", {}).items():
                    alg = LearningAlgorithm(alg_name)
                    self.bandit.counts[alg] = count
                self.bandit.total_count = bandit_stats.get("total_count", 0)
            
            safe_logger.info(f"Adaptive engine state loaded from {filepath}")
            
        except Exception as e:
            safe_logger.warning(f"Failed to load state: {e}")


# Global instance for singleton pattern
_adaptive_engines = {}

def get_adaptive_engine(user_id: str) -> AdaptiveEmotionalEngine:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    if user_id not in _adaptive_engines:
        _adaptive_engines[user_id] = AdaptiveEmotionalEngine(user_id)
    return _adaptive_engines[user_id]