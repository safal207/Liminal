version: '3.8'

services:
  # Оптимизированный Multi-LLM бэкенд
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.optimized
    volumes:
      - ./backend:/app
      - pip_cache:/root/.cache/pip
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:-demo_key}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-demo_key}
      - XAI_CACHE_SIZE=1000
      - XAI_ENABLE_SHAP=true
      - XAI_ENABLE_LIME=true
      - REDIS_HOST=redis
      - MULTI_LLM_FALLBACK_ENABLED=true
      - MULTI_LLM_CONSENSUS_THRESHOLD=0.7
      - MULTI_LLM_MODE=${MULTI_LLM_MODE:-demo}
    depends_on:
      - redis
      - xai
      - openai
      - claude

  # XAI сервис (Explainable AI)
  xai:
    build:
      context: ./services/xai
      dockerfile: Dockerfile.optimized
    volumes:
      - ./services/xai:/app
      - pip_cache:/root/.cache/pip
      - model_cache:/app/models
    ports:
      - "8001:8000"
    environment:
      - XAI_CACHE_SIZE=${XAI_CACHE_SIZE:-1000}
      - XAI_ENABLE_SHAP=${XAI_ENABLE_SHAP:-true}
      - XAI_ENABLE_LIME=${XAI_ENABLE_LIME:-true}
      - PYTHONUNBUFFERED=1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # OpenAI интеграция
  openai:
    build:
      context: ./services/openai
      dockerfile: Dockerfile.optimized
    volumes:
      - ./services/openai:/app
      - pip_cache:/root/.cache/pip
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:-demo_key}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4}
      - OPENAI_MAX_TOKENS=${OPENAI_MAX_TOKENS:-1000}
      - OPENAI_TEMPERATURE=${OPENAI_TEMPERATURE:-0.7}
      - PYTHONUNBUFFERED=1
    ports:
      - "8002:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Anthropic Claude интеграция
  claude:
    build:
      context: ./services/claude
      dockerfile: Dockerfile.optimized
    volumes:
      - ./services/claude:/app
      - pip_cache:/root/.cache/pip
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-demo_key}
      - CLAUDE_MODEL=${CLAUDE_MODEL:-claude-2}
      - CLAUDE_MAX_TOKENS=${CLAUDE_MAX_TOKENS:-1000}
      - CLAUDE_TEMPERATURE=${CLAUDE_TEMPERATURE:-0.7}
      - PYTHONUNBUFFERED=1
    ports:
      - "8003:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis для кеширования и обмена данными
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: ["redis-server", "--appendonly", "yes"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  # Prometheus для мониторинга
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    restart: always

  # Grafana для визуализации
  grafana:
    image: grafana/grafana:latest
    volumes:
      - ./grafana:/etc/grafana/provisioning
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    restart: always

volumes:
  redis_data:
  prometheus_data:
  grafana_data:
  pip_cache:  # Кеш для pip
  model_cache:  # Кеш для ML-моделей
